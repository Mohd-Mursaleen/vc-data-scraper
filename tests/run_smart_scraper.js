const fs = require('fs');
const ScraperAgent = require('../agents/ScraperAgent');
const CleanerAgent = require('../agents/CleanerAgent');
const StorageService = require('../services/StorageService');
const URLClassifier = require('../utils/URLClassifier');
require('dotenv').config();

async function runSmartScraper(researchOutputPath) {
  console.log('\nğŸš€ Starting Smart Scraper...\n');

  const researchData = JSON.parse(fs.readFileSync(researchOutputPath, 'utf-8'));
  
  const scraper = new ScraperAgent();
  const cleaner = new CleanerAgent();
  const storage = new StorageService();

  const firmName = 'Ascent Capital';
  const firmSlug = storage.getFirmSlug(firmName);

  console.log(`ğŸ“Š Processing: ${firmName}`);
  console.log(`ğŸ“ Slug: ${firmSlug}\n`);

  const allUrls = [
    ...(researchData.discovery?.urls || []),
    ...(researchData.news?.urls || [])
  ];

  console.log(`ğŸ“‹ Total URLs: ${allUrls.length}\n`);

  const { regular, linkedin } = URLClassifier.classify(allUrls);

  console.log(`âœ… Regular URLs: ${regular.length}`);
  console.log(`ğŸ”— LinkedIn URLs: ${linkedin.length}\n`);

  if (linkedin.length > 0) {
    storage.saveLinkedInQueue(firmSlug, linkedin);
  }

  console.log('ğŸŒ Processing Regular URLs...\n');

  const allDiscoveredLinks = [];

  for (let i = 0; i < regular.length; i++) {
    const urlObj = regular[i];
    const pageId = `page_${String(i + 1).padStart(3, '0')}`;

    console.log(`\n[${i + 1}/${regular.length}] Processing: ${urlObj.url}`);
    console.log(`   Context: ${urlObj.context}`);
    console.log(`   Importance: ${urlObj.importance}`);

    const scraped = await scraper.execute(urlObj.url);
    
    if (!scraped.html) {
      console.log(`   âŒ Failed to scrape (${scraped.error || 'unknown error'}), skipping...`);
      continue;
    }

    if (scraped.statusCode !== 200) {
      console.log(`   âš ï¸  HTTP ${scraped.statusCode}, skipping...`);
      continue;
    }

    const cleaned = cleaner.execute(scraped.html, urlObj.url);

    storage.savePage(
      firmSlug,
      pageId,
      scraped.html,
      cleaned
    );

    if (cleaned.links && cleaned.links.length > 0) {
      allDiscoveredLinks.push(...cleaned.links);
    }

    await new Promise(resolve => setTimeout(resolve, 1000));
  }

  await scraper.close();

  console.log('\n\nğŸ“Š SUMMARY');
  console.log('='.repeat(50));
  console.log(`âœ… Processed: ${regular.length} pages`);
  console.log(`ğŸ”— LinkedIn URLs queued: ${linkedin.length}`);
  console.log(`ğŸ” Total discovered links: ${allDiscoveredLinks.length}`);

  const discoveredLinkedIn = allDiscoveredLinks.filter(link => 
    URLClassifier.isLinkedIn(link.url)
  );
  
  if (discoveredLinkedIn.length > 0) {
    console.log(`   ğŸ“Œ Discovered LinkedIn profiles: ${discoveredLinkedIn.length}`);
    
    const allLinkedInUrls = [...linkedin, ...discoveredLinkedIn];
    storage.saveLinkedInQueue(firmSlug, allLinkedInUrls);
  }

  console.log(`\nâœ… Complete! Data saved to: ./data/firms/${firmSlug}/`);
}

const researchOutputPath = process.argv[2] || './research_output.json';

runSmartScraper(researchOutputPath);
